{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the gender of a name... We will use the heuristic that the last few characters in a name is its defining characteristic. For example, if the name ends with \"la\", it's most likely a female name, such as \"Angela\" or \"Layla\". On the other hand, if the name ends with \"im\", it's most likely a male name, such as \"Tim\" or \"Jim\". Using this program we will be able to compare the accuracy of classifiers that use varying numbers of ending letters to determine the gender of a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\jeevv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import nltk\n",
    "nltk.download('names')\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to extract the features from the input name\n",
    "# For this method, considering the last 2 letters will provide the most accuracy as we will see in the end\n",
    "def gender_features(word, num_letters = 2):\n",
    "    return {'feature': word[-num_letters:].lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of ending letters used: 1\n",
      "Classifier Accuracy (against training set) ==> 77.8%\n",
      "Dwight ==> male\n",
      "Andrew ==> male\n",
      "Jim ==> male\n",
      "Kelly ==> female\n",
      "Angela ==> female\n",
      "\n",
      "Number of ending letters used: 2\n",
      "Classifier Accuracy (against training set) ==> 82.19999999999999%\n",
      "Dwight ==> male\n",
      "Andrew ==> male\n",
      "Jim ==> male\n",
      "Kelly ==> female\n",
      "Angela ==> female\n",
      "\n",
      "Number of ending letters used: 3\n",
      "Classifier Accuracy (against training set) ==> 79.80000000000001%\n",
      "Dwight ==> male\n",
      "Andrew ==> male\n",
      "Jim ==> male\n",
      "Kelly ==> female\n",
      "Angela ==> female\n",
      "\n",
      "Number of ending letters used: 4\n",
      "Classifier Accuracy (against training set) ==> 70.8%\n",
      "Dwight ==> male\n",
      "Andrew ==> male\n",
      "Jim ==> male\n",
      "Kelly ==> female\n",
      "Angela ==> female\n"
     ]
    }
   ],
   "source": [
    "#Define main function using labeled training data\n",
    "if __name__ == '__main__':\n",
    "    #Extract labeled names\n",
    "    labeled_names = ([(name, 'male') for name in names.words('male.txt')]) + [(name, 'female') for name in names.words('female.txt')]\n",
    "    #seed random number generator and shuffle the training data\n",
    "    random.seed(9)\n",
    "    random.shuffle(labeled_names)\n",
    "    #Lets provide a list of input names to test our classifiers\n",
    "    input_names = ['Dwight', 'Andrew', 'Jim', 'Kelly', 'Angela']\n",
    "    #We don't know how many ending characters to consider, for this reason we will sweep the parameter space from 1 to 5 and each\n",
    "    #time we will extract the features\n",
    "    for i in range(1, 5):\n",
    "        print('\\nNumber of ending letters used:', i)\n",
    "        featuresets = [(gender_features(name, i), gender) for (name, gender) in labeled_names]\n",
    "        #splitting into train and test datasets\n",
    "        train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "        #Using Naive Bayes Classifier\n",
    "        classifier = NaiveBayesClassifier.train(train_set)\n",
    "        #Evaluate the classifier for each value in the parameter space\n",
    "        #Print classifier accuracy\n",
    "        print('Classifier Accuracy (against training set) ==>', str(100 * nltk_accuracy(classifier, test_set)) + str('%'))\n",
    "    \n",
    "        #Predict outputs for new inputs\n",
    "        for name in input_names:\n",
    "            print(name, '==>', classifier.classify(gender_features(name, i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the most accurate classifier is the second one. This classifier uses the two ending characters to make its predictions. The accuracy score is a measure of each classifier's accuracy against the test_set created from the second half of the shuffled featuresets(500 feature sets). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
